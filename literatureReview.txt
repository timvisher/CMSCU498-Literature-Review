Tim Visher
CMSCU 310
Literature Review
Due Date: 8-1-09

* Introduction

- This section should present an explanation of the topic being researched. It should include the _purpose_ of the research, a _thesis or problem statement_ and explain _the relevance and significance_ of the problem being researched. This section should also include an _overview_ of the literature review. You should include a few subheadings under this section

[[write]]

* Relevant Technologies and Historical Developments

The road to Cloud Computing is somewhat cyclical.  While the technologies that have developed recently and become widespread are new, the concepts are actually nearly as old as computing, or at the very least networked computing, itself.  To begin with, the principles of Cloud Computing have their roots in the Time Sharing Computing Model (TSCM) which was pioneered by [pioneer] at MIT. [citation]  The lack of widely available high-speed Internet access made the viability and applicability of this model short-lived but times have begun to change.  This section will cover in some detail the TSCM and its applicability to the investigation here-in.

Second, mobile computing has developed greatly within the past decade.  Technological developments have created smaller computers that are able to consume less power to do more work.  Also, high-speed wireless networking has become widely available in a variety of places from McDonalds to LAX.  At the same time, advances in mobile power had reached something of a plateau.  This section will cover the development of mobile power as it pertains to the adoption of Cloud Computing as the preferred model.

Third, this section will cover the beginnings of Cloud Computing.  What is Cloud Computing?  What bodies or corporations were central to its development?  These questions will be answered in this section.

** Time Sharing Computing Model

*** Description and Relevance

The first time-sharing computing system showed up at MIT in 1961.  "A time-sharing computer was one organized so that it could be used simultaneously by many users, each person having the illusion of being the sole user of the systemâ€”which, in effect, became his or her personal machine" (Campbell-Kelly, 2004).  Prior to the creation of time-sharing systems, the only way a computer could be efficiently used was through the batch processing model which required users to submit their work to a computer center where operators would decide how to keep the computer busy through efficiently scheduling the separate tasks.  It did keep the computers busy, but it was a difficult way to use the system because there was no immediate feedback to your actions.  The solution was proposed by a British Computer scientist named Christopher Strachey in 1959 and by John McCarthy at MIT around the same time: Allow multiple users to interact with a single mainframe computer through access terminals, using the time that they weren't directly using the CPU to process other people's jobs (Campbell-Kelly, 2004).  Today, all systems are in theory able to be used by multiple users at once over a network, but the practice is different because the computers are designed to be used by a single person with direct access to the machine.  

This is relevant to the topic of Cloud Computing and Ultramobile PC development because Cloud Computing is really a rehashing of the old computing-as-utility model that developed because of advances in time-sharing.  Until computers could be accessed by multiple users over a diverse geographical area, there was no way for computer utilities to exist.  However, other developments would have to happen before this model would become obsolete and then viable again.

*** Different Time Sharing Systems (TSS) and Their Characteristics

Several TSS were developed in the 1960s, which was the heyday of the time-sharing computing model.  The first was the Compatible Time-Sharing System (CTSS) developed at MIT under the guidance of Robert Fano and Fernando Corbato, which is considered to have been a proof of concept of sorts but never was acceptable for whole-sale public consumption.  It only allowed for 3 users to be signed on at one time and was difficult to use given that it required you to learn a language like FORTRAN which was of course acceptable in academia but was too much to ask of a normal human being (Campbell-Kelly, 2004).  The best known early system was the Dartmouth Time-Sharing System (DTSS).  It allowed for 30 concurrent users and made the important jump to the brand new BASIC programming language which for the first time allowed the masses to write their own programs to accomplish their information processing needs.  Finally, a system called Multics was developed at MIT in conjunction with General Electric and Bell Labs.  Multics could support 300 concurrent users and also used the BASIC language as well as allowing for programming in other languages (Levy, 1994).  Multics (and later Unix) is what allowed the Computer Utility to take flight. 

*** Rise of the PC and the Death of Time Sharing

Several factors contributed to the eventual collapse of the computer-as-utility model.  First, Grosch's Law became obsolete for the average computer user.  Grosch's law states that a computers power varies as a square of its price.  Because of this, it is more economical to have 25 people using a $1,000,000 machine rather than giving each individual a $200,000 machine.  Fortunately, hardware costs plummeted and made it possible to own a computer that was powerful enough to do anything you wanted and could be accessed only by you.  While Grosch's law holds true to this day, at the time it simply didn't matter (Campbell-Kelly, 2004).

The reason that it no longer mattered was two-fold.  First, technological developments outside of the computer industry proper allowed for the creation of computers that dwarfed the computational power of older machines and did it at a fraction of the cost.  The transition was from Vacuum Tubes down to Discrete Transistors and finally into Integrated Circuits.  Each down-step brought greater computational potential and reduced costs.  At the end of the 1960s, it was possible to buy a time-sharing system using something like Unix for its OS for a reasonable price and maintain control of it locally.  The reason this was so attractive is the other side of the issue (Campbell-Kelly, 2004).

Simply put, the corporations working so hard to deliver computer-as-utility could not put the software together to run their concept.  There was substantial interest, but after years of development a working system had not been expressed yet.  Eventually, GE pulled out of the industry altogether, believing that the software problem could not be solved.  IBM foundered for years with it's 360 series of computers.  At the same time, a free OS named Unix which grew out of the Multics effort was made available to power the smaller machines that were being built with new Integrated Circuit technology.  That software actually worked and was extremely solid.  The trend of reducing the cost of the hardware for doing ever increasingly complex calculations has continued under Moore's Law till now (Campbell-Kelly, 2004).  It has only been recently that the demand for computing power has once again opened the door for computing-as-utility.

** Rechargeable Batteries

*** Description and Relevance

There is one main strain of research regarding batteries for mobile computing and other such devices: Lithium-Ion Technology.  Lithium-Ion technology was first proposed by M. S. Whittingham while he was at Exxon in 1976.  At the time, the advance was revolutionary, allowing for a highly configurable battery shape which made it's applicability in the mobile electronics market very extensive and reducing the battery 'bleed' effect where a rechargeable battery will slowly loose its charge in storage.  In fact, Lithium-Ion batteries do not exhibit this effect at all, but they do age at a predetermined rate regardless of use, slowly reducing their ability to hold a charge.  

*** Relevant Mobile Computing Battery Research

Advances in Lithium-Ion Battery technology are primarily in 3 areas.  The first is in the area of safety.  The materials recommended by Whittingham were titanium sulfide and lithium metal.  It turned out that batteries using lithium metal are extremely dangerous.  The first advances were in the material used.  If you forgo the use of lithium metal in favor of another material that contains lithium ions, you get a much more stable battery [citation].  Second, the composition of the materials (known as the anode and the cathode) is being changed experimentally to increase the amount of cycles the battery has and to improve its power output [citation].  Finally, advances have been made in the structure of the anode and cathode to allow for greater surface area.  This allows for the battery to have a higher capacity, as the surface area of the structure of the lithium materials is the primary barrier to higher capacity batteries [citation].

*** Current Trends

[[write]]

** Cloud Computing

*** Description and Relevance

Cloud Computing is a relatively new trend in computing services that capitulates on old computing theories that have finally had hardware and software catch up to them.  As a definition, Cloud Computing is the use of outsourced hardware for doing your computing via the Internet.  Combined with broadband Internet access, Cloud Computing has become one of the largest revolutions in computer history, transforming the way people think about their computers and how they should be used.  Principally, application suites such as the offerings from Google have modified the thought that having control over the software you use day to day is a necessity.  Whereas the original shift from faith in a centralized system to belief that personal control was better came from the inherent flaws in the software being produced and the hardware being designed, today's world sees software reaching a very mature state and Internet connections finally being up to speed enough to provide near desktop-like experiences.  The off loading of the need for computing power to something over the Internet could signal extreme changes in the design of mobile computers.

*** Development

Cloud Computing has its roots in the Time-Sharing Computer Utility model that tried to take flight in the 1960s.  However, many of the problems encountered then have been solved (massive concurrent access, interface issues, Internet (Broadband) access, etc.).  Amazon is widely considered to be the progenitor of Cloud Computing in its modern form.  They expressly take the utility approach to computing in the Cloud.  The development of the service was organic in that the company needed a very high-powered computer system in order to deal with its burgeoning success in the online sales industry.  Once it had developed the system it needed, the company's founder Jeff Bezos realized that the potential was there to open the system up in a metered way to the rest of the world.  This was done both for monetary purposes (as the computing utility business model is quite lucrative) and because it was a practical way for getting more information into Amazon's systems.

*** Application

Today, Amazon and many others such as Yahoo [citation], Google [citation], and Microsoft [citation] are beginning to offer Cloud Computers under various guises.  Google, for instance, offers many software services for free (such as a Mail Client, a Document editor, and a Spread Sheet editor) while they insert their ad service into them [citation].  Beyond just the cloud as a computer, there is a whole new breed of software termed Software as a Service (SAAS) which offers no install, enterprise grade, software services delivered via the cloud.  While this is not necessarily 'traditional' cloud computing, it does offload the hardware and software requirements for computer solutions to a central server and off of your local hardware installation.  

** Thin-Clients

"Thin-client Computing is the modern version of the old time-sharing approach" (Schmidt, 1999).  The advent of ubiquitous high-speed networks has allowed the development of dumb or thin-client terminals that access powerful centralized servers and execute their applications on the server.  This has several effects, such as a reduced total cost of ownership through the lack of a need to upgrade the client hardware on a regular basis, ease of administration with only a few centralized computing resources that you can control directly, etc.  

*** Definition

Thin-Clients are, in short, computing systems that consist of a server which runs all logic and processing requirements of an application and a Client which displays the output only from the remote server of a Remote Display Protocol (RDP) (Lai, 2002).  An advantage of thin-client architecture for doing remote computing is that most applications can be run as they are, without any rewrite to take advantage of a remote protocol, because the display protocol is implemented at a lower level in the OS Hierarchy than the applications execute (Schmidt, 1999).  Thus, instead of a sending the display information to a local monitor, you instruct the OS to send its display information instead over the network to a client that has requested that you send said information to it.  Anyone with network access can request to utilize the computing resources of a central server that has implemented the server protocol of the thin-client and from anywhere at any time remotely access their session as if they were sitting at the machine.  

In the pure sense of the word, a simple text-only protocol such as telnet or ssh is a thin-client system.  They leave the processing and computing responsibility up to the server and simply request that the text be sent back to them rather than displayed on the server's local interface.  This worked for a large number of applications for a number of years but is no longer sufficient in today's GUI environment.  However, the concept still holds constant: Transport remote input to the server over the network, execute logic, transfer display back to the client over the network.  Also, any system today can behave as a thin-client.  For instance, any laptop running an Apple, Microsoft, or Linux OS has VNC clients and servers written for them.  However, in the sense that we are concerned with, only a computer that is nothing but a dumb terminal is interesting.  These clients can be configured to address a remote server over the network but apart from the access to the server, they can do nothing.  All they do is implement the client protocol so that they can display the information being sent to them.

A clear advantage cited by many as an important reason to utilize thin-client computers is the centralization of maintenance and upkeeping.  The obvious disadvantage of a PC in the context of a corporation or other group of people is that it's hard to control what's installed and to upgrade the hardware.  To a great extent, every machine must in effect be administered separately.  Though tools have arisen that make this somewhat easier, there is still a challenge associated with administering a distributed network of PCs.  Often, the cost of upgrading everyone's PCs renders hardware 'obsolete before it has been paid for". (Schmidt, 1999)  The use of thin-client systems allows maintenance, security, and configuration to be done at a clear centralized point rather than across a disparate, ever changing network.  

*** Current Research

So far, thin-client systems have primarily penetrated in the medical, banking, education, government, and sales industries. Still, they only account for less than 1% of the total market share.  However, experts believe that trends indicate that they will rapidly grow in market share, especially in the Enterprise market. (Tynan, 2005)  

Current pushes in research are focused mainly on figuring out how to get thin-clients to work well over wide-area networks such as the Internet and Internet2.  In order to do this, researchers are attempting to understand what the performance bottlenecks are and how they can be overcome.  The designers of the SLIM protocol, which is a Sun product, believe that the bottleneck exists mainly in requiring too much communication between the client and the server; rather than keeping the client completely dumb, most thin-client systems try to keep a foot in both worlds, requiring some state to be maintained by the client and some by the server, which occasionally requires multiple round trips to the server in order to accomplish simple operations (Lai, 2007). (Schmidt, 1999)  By design, Microsoft appears to believe that the bottle neck is bandwidth (Lai, 2007).  Lai and Nieh of Colombia University indicate that the real performance hindrance is network latency and choice of lower order display primitives or application level programming interfaces (2007).  

*** Relevance

Thin-clients are applicable to my current research project in that the design of a mobile hardware platform that is fine-tuned for taking advantage of Cloud services would have a great deal of similarity with today's thin-clients.  Very little processing power would have to exist on the client side as long as they had a high-bandwidth connection to the application server.  The question would only be how thin you would make the clients.

** Web 2.0

A central development that's helping to pave the way for netbook computers that are streamlined for thin-client like computation utilizing Cloud Services is the advent of what has come to be known as Web 2.0.  Tim O'Reilly, in his seminal paper on the subject, describes Web 2.0 "as a set of principles and practices that tie together a veritable solar system of sites that demonstrate some or all of those principles." The principles he is referring to are things such as an emphasis on deep customer collaboration, offering of your applications not as delivered software but as web services, and an emphasis on multiple light-weight UIs to the same data, especially the web connected devices.

*** The Web is the Platform

The "Web is the Platform" phrase is just ambiguous enough that it's incredibly easy to misinterpret.  Tim O'Reilly notes this as well when he states that the term has been adopted by "buzz-word addicted start-ups" with "no real understanding of what it means".  The Web is the Platform, at its core, means the use of the Internet to deliver your service to customers (Miller).  This is in stark contrast to the model of 'shrink wrapped software' where a long development cycle is concluded with a monolithic release that contains maybe a years worth of work available for repurchasing.  In that model, a customer would buy a license to use a particular release of your software forever and would only purchase a new version if the features warranted it.  So in a strict sense if you use the Web to deliver your software then you are using the Web as a platform.  However, you would not be very Web 2.0.

To be Web 2.0, you must also pursue an aggressive release schedule which delivers new features that are ready to be used on a regular basis to your customers (Miller).  This is what advocates of Web 2.0 say is the advantage of the Web as Platform; the ability to deliver features to your audience at a much higher rate (O'Reilly).

*** Key Properties

-- TODO Incorporate Alexander's article into citations

That being said, there are many key properties that have come to define Web 2.0.  Going back to O'Reilly, software vendors are judged on a continuum regarding their adherence to the general principles of Web 2.0 rather than being strictly 'in' or 'out'.  Here I will define the core principles of Web 2.0 that have been identified already.

**** Collaboration

There is a massive emphasis on collaboration at many levels in Web 2.0 (O'Reilly).  

At the developer/vendor level, there is in emphasis on the use of open tools and technologies to promote the maximum amount of interoperability possible without literal reproduction of services (Miller).  For example, the social bookmarking site Delicious.com allows anyone to access their data stream for any purpose (with the exception that you are not to create a search engine using it).  This could be as simple as including your latest delicious bookmarks on the homepage of your blog or as complex as designing a popular tag portal that includes advertisements based on the most popular tags clicked.  The point is, Delicious.com opens up their data rather than creating a proprietary model that attempts to lock out other services.  

At the customer level, there is a new push for user generated content.  In the words of Bryan Alexander, "these sections of the Web break away from the page metaphor. Rather than following the notion of the Web as book, they are predicated on microcontent."  Another way to put this would be for a shift from one-way delivery of content to two-way content production participation (Alexander).   A key feature of Web 1.0 was that the customer was seen primarily as a consumer of content rather than a participant in its creation.  So even if you had a dynamically created page with a database back-end etc., you would still be in control of all of the variables in order to create the user experience you desired.  Then, customers would use your service if they liked what you did or they would not.  In Web 2.0, you let the user have a much larger degree of control on their own experience with your service.

O'Reilly gives an example of this at the time his paper was written: Barnes and Noble vs. Amazon.  Barnes and Noble does not offer less books than Amazon and it's less or more well known, but Amazon consistently out sells Barnes and Noble through their online store.  O'Reilly believes that this is because Amazon specifically encourages user participation in the form of product reviews, wish lists, product recommendation lists (viewable by the public), and not the least of which the use of their opinion to form search results.  Whereas a search at Barnes and Noble often "lead[s] with the company's own products, or sponsored results, Amazon always leads with "most popular", a real-time computation based not only on sales but other factors that Amazon insiders call the "flow" around products" (O'Reilly).

**** Openness

Closely associated with collaboration, the principle of openness is primarily in reference to data.  In the seven discreet core competencies of Web 2.0 organizations that O'Reilly documents, he lists "control over unique, hard-to-recreate data sources that get richer as more people use them."  However, control here does not imply hiding.  Instead, it means that you control the interfaces by which the data can be manipulated and added to. Again in the case of Delicious, they have defined APIs for adding new bookmarks, accessing current bookmarks, accessing popular bookmarks, etc. that allow others to create new views into that data that customers may find add enough value to the raw data that they are willing to pay for it.  

Of course, the other part of the quote indicates that the data gets better as more people uses it.  Services like Delicious, Reddit, and Digg are so valuable _because_ of the amount of people using them.  It would be possible to create a Delicious application that only I could use on my personal machine (i.e. the bookmarking feature of your browser), but that wouldn't be very valuable outside of my own personal use.  But if 100,000 people are all using your bookmarking service, suddenly the information that can be derived from the service is much more interesting to a much wider audience.  And obviously, the more popular your particular service is, the harder it becomes to reproduce your data set because the market has shrunk.  In the words of O'Reilly, "the race is on to own certain classes of core data."  Once you own the data, then you can expose that data to others and allow them to create other interfaces.

**** Mashup Software

A third principle at the core of Web 2.0 is the delivery of software quickly with an emphasis on streamlining for a specific purpose rather than creating the next Microsoft Office or Adobe Creative Suite (Miller).  A prime example of this would be Google Maps.  Maps has an extremely comprehensive data back end but the interface to that data that Google designed is focused tightly on getting directions to and from locations and viewing data about locations and routes.  No attempts are made to shoehorn atlas type data or historical information onto the location.  However, they have exposed an API to that map data and other people or businesses are free to attempt to build that information over the map information.  Thus, the business model of today's Web 2.0 corporations is adding value to data through the interface.  

Because the sources of data are exposing it to other businesses and to the public, it's possible to create what has become known as mashup software.  This is software that doesn't create any data of its own, but instead aggregates data in an interesting way that may or may not be more useful to the customer.  One company provides a comprehensive map database with an API, another company provides an atlas service, and another provides a location based history service.  Finally, a fourth company comes along and does nothing but provide a mashup of the 3, allowing you to browse a location, view statistics such as population density, and look at historical highlights.  That's the heart of mashup software and a significant part of what defines Web 2.0.

*** Specific Applications

There are two business domains that have taken a special interest in Web 2.0 as a way to enhance what they are offering to their customers: The Medical Community and the Library Community.

**** Medical 

Dean Giustini discusses how changes brought about by Web 2.0 could be utilized to greatly change the way the Medical community serves its customers in his editorial in the BMJ.  He envisions blogs as a way to help the medical community skim through the massive volumes of information published by their colleagues in a focused way.  Citing one especially notable blog, Clinical Cases and Images, he notes that not only is it a way to get a lot of information cleanly and easily, but it also allows for the medical community to engage with the information instantaneously.  The prominent Web 2.0 technology RSS is also noted as a way for the medical community to engage with information in a simple and organized fashion.  Finally, he notes that a true success would be accomplished if the medical community would embrace a wiki technology and engage with one another in the building up of medical knowledge that is accessible to all.  This would be a massive shift in the attitude of the medical community which today is focused on secrecy more than collaboration.  However, he feels that a wiki edited and controlled by the experts of the field could possibly "create optimal knowledge build opportunities for doctors."

**** Libraries

Paul Miller discusses how Web 2.0 philosophies can be applied in the Library community in his Ariadne article.  The core of the argument is that instead of fearing the inherent openness and collaborative nature of the Web 2.0 philosophy, Libraries and Librarians should instead embrace these philosophies to help their customer and each other much more effectively.  Libraries are already uniquely positioned with highly specialized and unique data sources that until now have been controlled completely by them.  To start, Miller proposes an effort to at least build collaboration between Libraries (an effort which has already begun to take shape in our day with services such as that offered by SirsiDynix for a global library catalog).  This would be more along the lines of business to business collaboration, but would begin to help Libraries behave in a more Web 2.0 way.  Finally, he makes the suggestion that while building inter-library collaboration is a good start, Libraries need to utilize every applicable service to make their incredible wealth of data easily available to the customer.

** Ultra-Mobile PCs / Netbooks / Smart-Phones

[[write]]

** Internet2 / Broadband

[[write]]

* Summary/Conclusions

- This section should show the connection and reasons for the previous sections by providing an overall summary of the literature review. Any conclusions that can be drawn from the research should be stated here. Be sure to back up your conclusions with references from the literature. 
- You will probably not have subheadings in this section, but the summary should clearly pull together all the sections in the paper. It should be several paragraphs long.

[[write]]

* References

- Every citation must have a reference in and every reference in this section is cited in your paper.

[[write]]

