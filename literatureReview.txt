The Effects of Web 2.0 and Cloud Computing on the Design of Mobile Computing Hardware.
Tim Visher
CMSCU 310
Due Date: 01-20-09

* Introduction

The purpose of this project is to discover the attributes of mobile computing
hardware specifically designed to take advantage of the current resurgence in
computing utilities collectively known as Cloud Computing.  The primary
motivation is that today's Netbook and ultra-mobile PC offerings are still being
designed within a desktop computing frame of mind where most of the work the
computer is going to do is to be taken care of locally.  It is believed that
there is a better design model for these computers in this new environment.  The
author's thesis is that there is a certain performance barrier beyond which the
consumer public will be willing to readopt the thin-client, time-sharing
computer model, that barrier being a combination of usable life of the device
between charges and optimization for basic tasks performed locally as well as
optimum performance for services accessed via the Internet.

** Relevance and Significance

Amazon and others high-profile technology companies have already begun to offer
a wide variety of cloud services designed to place the brunt of the
computational and storage related burden on computers on the network rather than
on the customers' desktop machine (Reiss, 2008).  These cloud computing services
are giving rise to a new class of applications, coined collectively by Tim
O'Reilly as Web 2.0, that focus on delivering effective software solutions via
the Internet rather than distributing them via shrink-wrapped packages
(Bleicher, 2006).  In tandem with and because of this development, companies
such as ASUS are beginning to offer what have become known as netbooks or
ultra-mobile PCs.  These computers are stripped down versions of more
traditional notebook computers, in both power and size.  They are designed to
surf the web and do many of the other common things that computer users do
(e-mail, word processing, light gaming) while being very portable.  Also,
devices like the iPhone, Blackberry, and Palm, collectively known as
smart-phones, are revolutionizing the way people surf the web and think about
computing access.  GMail, Google's popular web based e-mail client, has a
version specifically designed to work with such small scale mobile computing
devices (as smart-phones are really much more than phones now) in deference to
the fact that many customers desire to be able to access the services they use
on the Web from anywhere at any time.  At the same time, efforts are being made
to give the experience of a desktop application to all mobile users, an effort
that is very hard to achieve on today's computing platforms (especially
smart-phones).

The fact is, the move to the Cloud Computing paradigm is happening and is just
waiting for someone to create a hardware platform that is truly optimized for
that environment.

** Topical Overview

In light of the above situation, this literature review will be focused on a
variety of topics all germane to the development of ultra-mobile computers
specifically designed for long, cell-phone like inter-charge life expectancies
and the use of cloud services.  One historical development that needs to be
covered is Time Sharing.  Time Sharing was a computing model born in the days
when computer hardware was very expensive.  The idea was that a computer's
computational time could be split between many users accessing the machine over
dumb terminals that did nothing but display output and send input.  The idea
never really took off in the consumer sector, but it appears to making its
comeback now.  In conjunction with this is the concept of a thin-client, which
despite the relative lack of a success of the time sharing computing model has
been very successful, especially in the medical space.  Thin-Client hardware is
basically a dumb terminal optimized to display output from a centralized server
which may run any kind of operating system.  A great deal of research has been
done on the best way to implement thin-client architectures, but the field is
alive and well and very relevant to the development of ultra-mobile PCs.

Rechargeable batteries play a significant role in almost every mobile electronic
device on the market today.  Often, battery life is the key component in the
consumers decision making process.  The iPhone was criticized harshly until a
firmware update enabled customers to get a day's worth of use out of it between
charges.  Google's G1 sank under similar criticisms.  Today's ultra-mobiles
don't offer battery life expectancy much greater than today's notebooks.  The
success of smart-phone technology over the ultra-mobile at the moment mainly
hinges on this distinction.

Cloud Computing, Web 2.0, and the developments concerning broadband Internet
access and Internet2 are also topics this literature review will be covering.
Cloud Computing is, as stated, a rehashing of the old time sharing model only
with much higher bandwidth and much broader access.  Web 2.0 is a loose
consortium of ideals that many of the companies providing their software as a
service adhere to (O'Reilly, 2007).  Broadband Internet is self explanatory.
These three technologies together provide the basis for why an ultra-mobile,
thin-client PC could be possible now when it could not have been 10 or 20 years
ago.  These technologies and the literature associated with them will be covered
in the following sections.

* Relevant Technologies and Historical Developments

The road to Cloud Computing is somewhat cyclical.  While the technologies that
have developed recently and become widespread are new, the concepts are actually
nearly as old as computing, or at the very least networked computing, itself.
To begin with, the principles of Cloud Computing have their roots in the Time
Sharing Computing Model (TSCM) which was pioneered by John McCarthy at MIT
(Levy, 1994).  The lack of widely available high-speed Internet access made the
viability and applicability of this model short-lived but times have begun to
change.  This section will cover in some detail the TSCM and its applicability
to the investigation here-in.

Second, mobile computing has developed greatly within the past decade.
Technological developments have created smaller computers that are able to
consume less power to do more work.  Also, high-speed wireless networking has
become widely available in a variety of places from McDonalds to LAX.  At the
same time, advances in mobile power had reached something of a plateau.  This
section will cover the development of mobile power as it pertains to the
adoption of Cloud Computing as the preferred model.

Third, this section will cover the beginnings of Cloud Computing.  What is Cloud
Computing?  What bodies or corporations were central to its development?  These
questions will be answered in this section.

** Time Sharing Computing Model

*** Description and Relevance

The first time-sharing computing system was developed at MIT in 1961.  "A
time-sharing computer was one organized so that it could be used simultaneously
by many users, each person having the illusion of being the sole user of the
system---which, in effect, became his or her personal machine" (Campbell-Kelly,
2004).  Prior to the creation of time-sharing systems, the only way a computer
could be efficiently used was through the batch processing model which required
users to submit their work to a computer center where operators would decide how
to keep the computer busy through efficiently scheduling the separate tasks.  It
did keep the computers busy, but it was a difficult way to use the system
because there was no immediate feedback to the customer's actions.  The solution
was proposed by a British Computer scientist named Christopher Strachey in 1959
and by John McCarthy at MIT around the same time: Allow multiple users to
interact with a single mainframe computer through access terminals, using the
time that they weren't directly using the CPU to process other people's jobs
(Campbell-Kelly, 2004).  Today, all systems are in theory able to be used by
multiple users at once over a network, but the practice is different because the
computers are designed to be used by a single person with direct access to the
machine.

This is relevant to the topic of Cloud Computing and Ultramobile PC development
because Cloud Computing is really a rehashing of the old computing-as-utility
model that developed because of advances in time-sharing.  Until computers could
be accessed by multiple users over a diverse geographical area, there was no way
for computer utilities to exist.  However, other developments would have to
happen before this model would become obsolete and then viable again.

*** Different Time Sharing Systems (TSS) and Their Characteristics

Several TSS were developed in the 1960s, which was the heyday of the
time-sharing computing model.  The first was the Compatible Time-Sharing System
(CTSS) developed at MIT under the guidance of Robert Fano and Fernando Corbato,
which is considered to have been a proof of concept of sorts but never was
acceptable for whole-sale public consumption.  It only allowed for three users
to be signed on at one time and was difficult to use given that it required the
customer to learn a language like FORTRAN which was of course acceptable in
academia but was too much to ask of a normal human being (Campbell-Kelly, 2004).
The best known early system was the Dartmouth Time-Sharing System (DTSS).  It
allowed for 30 concurrent users and made the important jump to the brand new
BASIC programming language which for the first time allowed the masses to write
their own programs to accomplish their information processing needs.  Finally, a
system called Multics was developed at MIT in conjunction with General Electric
and Bell Labs.  Multics could support 300 concurrent users and also used the
BASIC language as well as allowing for programming in other languages (Levy,
1994).  Multics (and later Unix) is what allowed the Computer Utility to take
flight.

*** Rise of the PC and the Death of Time Sharing

Several factors contributed to the eventual collapse of the computer-as-utility
model.  First, Grosch's Law became obsolete for the average computer user.
Grosch's law states that a computers power varies as a square of its price.
Because of this, it is more economical to have 25 people using a $1,000,000
machine rather than giving each individual a $200,000 machine.  Fortunately,
hardware costs plummeted and made it possible to own a computer that was
powerful enough to do anything most typical users wanted and could be accessed
only by the computer's owner.  While Grosch's law holds true to this day, at the
time it simply didn't matter (Campbell-Kelly, 2004).

The reason that it no longer mattered was two-fold.  First, technological
developments outside of the computer industry proper allowed for the creation of
computers that dwarfed the computational power of older machines and did it at a
fraction of the cost.  The transition was from vacuum tubes down to discrete
transistors and finally into integrated circuits.  Each down-step brought
greater computational potential and reduced costs.  At the end of the 1960s, it
was possible to buy a time-sharing system using something like Unix for its OS
for a reasonable price and maintain control of it locally.  The reason this was
so attractive is the other side of the issue (Campbell-Kelly, 2004).

The corporations working so hard to deliver computer-as-utility could not put
the software together to run their concept.  There was substantial interest, but
after years of development a working system had not been expressed yet.
Eventually, GE pulled out of the industry altogether, believing that the
software problem could not be solved.  IBM floundered for years with its 360
series of computers.  At the same time, a free OS named Unix which grew out of
the Multics effort was made available to power the smaller machines that were
being built with new integrated circuit technology.  That software actually
worked and was extremely solid.  The trend of reducing the cost of the hardware
for doing ever increasingly complex calculations has continued under Moore's Law
till now (Campbell-Kelly, 2004).  It has only been recently that the demand for
computing power has once again opened the door for computing-as-utility.

** Thin-Clients

"Thin-client Computing is the modern version of the old time-sharing approach"
(Schmidt, 1999).  The advent of ubiquitous high-speed networks has allowed the
development of dumb or thin-client terminals that access powerful centralized
servers and execute their applications on the server.  This has several effects,
such as a reduced total cost of ownership through the lack of a need to upgrade
the client hardware on a regular basis, ease of administration with only a few
centralized computing resources that can be controlled directly, etc.

*** Definition

Thin-Clients are, in short, computing systems that consist of a server which
runs all logic and processing requirements of an application and a client which
displays the output only from the remote server of a Remote Display Protocol
(RDP) (Lai, 2002).  An advantage of thin-client architecture for doing remote
computing is that most applications can be run as they are, without any rewrite
to take advantage of a remote protocol, because the display protocol is
implemented at a lower level in the OS hierarchy than the applications execute
(Schmidt, 1999).  Thus, instead of a sending the display information to a local
monitor, operators of an RDP server instruct the OS to send its display
information instead over the network to a client that has requested said
information to be sent to it.  Anyone with network access can request to utilize
the computing resources of a central server that has implemented the server
protocol of the thin-client and from anywhere at any time remotely access their
session as if they were sitting at the machine.

In the pure sense of the word, a simple text-only protocol such as telnet or ssh
is a thin-client system.  They leave the processing and computing responsibility
up to the server and simply request that the text be sent back to them rather
than displayed on the server's local interface.  This worked for a large number
of applications for a number of years but is no longer sufficient in today's GUI
environment.  However, the concept still holds constant: Transport remote input
to the server over the network, execute logic, transfer display back to the
client over the network.  Also, any system today can behave as a thin-client.
For instance, any laptop running an Apple, Microsoft, or Linux OS has VNC
clients and servers written for them.  However, in the sense that we are
concerned with, only a computer that is nothing but a dumb terminal is
interesting.  These clients can be configured to address a remote server over
the network but apart from the access to the server, they can do nothing.  All
they do is implement the client protocol so that they can display the
information being sent to them.

A clear advantage cited by many as an important reason to utilize thin-client
computers is the centralization of maintenance and upkeep.  The obvious
disadvantage of a PC in the context of a corporation or other group of people is
that it's hard to control what's installed and to upgrade the hardware.  To a
great extent, every machine must in effect be administered separately.  Though
tools have arisen that make this somewhat easier, there is still a challenge
associated with administering a distributed network of PCs.  Often, the cost of
upgrading everyone's PCs renders hardware "obsolete before it has been paid
for". (Schmidt, 1999) The use of thin-client systems allows maintenance,
security, and configuration to be done at a clear centralized point rather than
across a disparate, ever changing network.

*** Current Research

So far, thin-client systems have primarily seen market penetration in the
medical, banking, education, government, and sales industries. Still, they
account for less than 1% of the total market share.  However, experts believe
that trends indicate that they will rapidly grow in market share, especially in
the Enterprise market. (Tynan, 2005)

Current pushes in research are focused mainly on figuring out how to get
thin-clients to work well over wide-area networks such as the Internet and
Internet2.  In order to do this, researchers are attempting to understand what
the performance bottlenecks are and how they can be overcome.  The designers of
the SLIM protocol, which is a Sun product, believe that the bottleneck exists
mainly in requiring too much communication between the client and the server;
rather than keeping the client completely dumb, most thin-client systems try to
keep a foot in both worlds, requiring some state to be maintained by the client
and some by the server, which occasionally requires multiple round trips to the
server in order to accomplish simple operations (Lai, 2002). (Schmidt, 1999) By
design, Microsoft appears to believe that the bottle neck is bandwidth (Lai,
2002).  Lai and Nieh of Colombia University indicate that the real performance
hindrance is network latency and choice of lower order display primitives or
application level programming interfaces.

*** Relevance

Thin-clients are applicable to this research project in that the design of a
mobile hardware platform that is fine-tuned for taking advantage of Cloud
services would have a great deal of similarity with today's thin-clients.  Very
little processing power would have to exist on the client side as long as they
had a high-bandwidth connection to the application server.  The only remaining
question would be how thin the clients should be.  With nothing but an Internet
connection and video card, the machine would be worthless without the Network,
but it would in theory last a very long time and have access to enormous
computing resources far outweighing the capabilities it could have locally.

** Netbooks / Smartphones

While there isn't really a specific set of technologies that are lending to the
development of Netbooks and Smartphones other than the miniaturization of parts
found commonly in today's computers, these devices play a major role in the
development of mobile hardware designed specifically to take advantage of
Internet based services.  Consumers have begun to realize that what they need
out of a computer can, for the most part, be provided by free and
computationally inexpensive solutions in today's cloud computing market.  That
has created a market for relatively cheap, low powered devices that are focused
on providing a solution in that the consumer computing services space.

*** Netbooks

Netbooks are a step forward in the specialized market of consumers who do not
make heavy demands on their computer hardware but instead primarily use the
Internet and do basic word processing and light gaming.  It turns out that this
is a surprisingly high percentage of customers, if Amazon's sales statistics as
of October, 2008 are any indicator; 13 of the top 15 notebook computers sold
there were netbooks at that time (Copeland, 2008).  It is important to
emphasize, as we will be noting of Smartphones as well, that netbooks are not
new in terms of technological innovation.  They are, rather, a response to a
perceived reality in the computing consumer market which is that most people
only really need to access the Internet to do most of what they want to do.

According to Paul Bergevin, an executive at Intel, netbooks are "small laptops
that are designed for wireless communication and access to the Internet" (2006).
They're something of a "cross between a Blackberry and a full-blown laptop"
(Copeland, 2008) that meet the consumer's expectation of a mobile device that
they can use like they would a laptop (in contrast to a smart phone which is
used quite differently due to the constrictive hardware) for accessing the
Internet from a variety of locations.  There is also an emphasis on being low
cost, about as much or less as the more expensive Smartphones.  As with
Smartphones and today's notebook computers, there are data plans available from
major telecoms companies such as AT&T to also allow access to the Internet via
today's wireless networks, albeit not exactly broadband connectivity (AP, 2009).
Netbooks are a key development in the mobile computing market that are a step
towards thin-client like mobile hardware designed to take advantage of the
cloud.

*** Smartphones

Smartphones are the logical step beyond plain-vanilla mobile phones and PDAs.
In the words of one analyst quoted in Jo Best's article describing Smartphones,
Smartphones are "a large-screen, data-centric, handheld device designed to offer
complete phone functions whilst simultaneously functioning as a personal digital
assistant (PDA)" (2006).  Best continues, saying that this definition may not go
far enough, as another common attribute of Smartphones today are that they
implement an open operating system that third party developers can provide
applications to run on.  For example, Apple's iPhone provides a third party SDK
and an application store through which iPhone customers can obtain the
applications.  Google's G1 has a very similar feature set, though with a
different API.

Another primary capability that most Smartphones are beginning to develop is
fully-capable web browsing on the phone.  The iPhone's Safari browser
implementation utilizes zooming technology to render a web page as if the screen
were larger, allowing the user then to zoom in on the interesting parts.  The
G1's mobile Chrome implementation does similar things and other Smartphones as
well.  Also, they utilize the many different data networks to access the
Internet so that the Internet is available anywhere mobile access is as well,
allowing customers to get to their online data through either applications
created for the phone or via the browser.  Overall, Smartphones do not represent
anything truly revolutionary technology wise; they're more a mashup of old
technologies much like the evolution of Web 2.0.  However, they represent a core
strategy that could be used to provide a truly mobile computing platform.

** Rechargeable Batteries

*** Description and Relevance

There is one main strain of research regarding batteries for mobile computing
and other such devices: Lithium-Ion Technology.  Lithium-Ion technology was
first proposed by M. S. Whittingham while he was at Exxon in 1976.  At the time,
the advance was revolutionary, allowing for a highly configurable battery shape
which made its applicability in the mobile electronics market very extensive and
reducing the battery 'bleed' effect where a rechargeable battery will slowly
lose its charge in storage.  In fact, Lithium-Ion batteries do not exhibit this
effect at all, but they do age at a predetermined rate regardless of use, slowly
reducing their ability to hold a charge.

*** Relevant Mobile Computing Battery Research

Advances in Lithium-Ion Battery technology are primarily in three areas.  The
first is in the area of safety.  The materials recommended by Whittingham were
titanium sulfide and lithium metal.  It turned out that batteries using lithium
metal are extremely dangerous.  The first advances were in the material used.
Forgoing the use of lithium metal in favor of another material that contains
lithium ions yields a much more stable battery (Samar, 1981).  Second, the
composition of the materials (known as the anode and the cathode) is being
changed experimentally to increase the amount of cycles the battery has and to
improve its power output.  In one case, that of LG Chemical's CPI subsidiary,
the researchers are using manganese cathodes to allow a much more effective
structure of electrodes in the battery.  This improvement gives them a longer
calendar life, higher energy density, and greater battery safety.  In 1996, a
team lead by John Goodenough, a major innovator in the secondary battery space,
developed another popular cathode material, LiFePO4.  Finally, advances have
been made in the structure of the anode and cathode to allow for greater surface
area.  This allows for the battery to have a higher capacity, as the surface
area of the structure of the lithium materials is the primary barrier to higher
capacity batteries (Idota, 1997).

*** Current Trends

Currently, improvements to Lithium Ion technology specifically still deal
primarily with the three main areas: better anode cathode structure, better
materials for use as the anode or the cathode, and safer batteries able to
tolerate higher variability in the operational environment.  A group at MIT
claimed in 2006 to have developed a bio-technology whereby viruses are used to
create nano-sized wires, obviously having a tremendous potential to greatly
increase the capacity of Lithium Ion batteries (Nam, 2006).  In 2007,
researchers at Stanford successfully created a battery using this technology
that had 10 times the energy density of a conventional Lithium-Ion battery
(Stanford, 2007).  Also, other groups are investigating other cathode materials
like A2FePO4F citing some of the inherent problems with olivine structures such
as comparatively short theoretical life expectancies.  New cathode materials
overcome these inherent shortcomings in olivine structures (Ellis, 2007).  These
trends and others are enabling the battery life expectancy of electronic devices
to increase greatly.

** Cloud Computing

*** Description and Relevance

Cloud Computing is a relatively new trend in computing services that capitulates
on old computing theories that have finally had hardware and software catch up
to them.  As a definition, Cloud Computing is the use of outsourced hardware for
doing computing via the Internet.  Combined with broadband Internet access,
Cloud Computing has become one of the largest revolutions in computer history,
transforming the way people think about their computers and how they should be
used.  Principally, application suites such as the offerings from Google have
modified the thought that having control over the software many people use on a
daily basis a necessity.  Whereas the original shift from faith in a centralized
system to belief that personal control was better came from the inherent flaws
in the software being produced and the hardware being designed, today's world
sees software reaching a very mature state and Internet connections finally
being up to speed enough to provide near desktop-like experiences.  The off
loading of the need for computing power to something over the Internet could
signal extreme changes in the design of mobile computers.

*** Development

Cloud Computing has its roots in the Time-Sharing Computer Utility mode that
tried to take flight in the 1960s.  However, many of the problems encountered
then have been solved (massive concurrent access, interface issues, Internet
(Broadband) access, etc.).  Amazon is widely considered to be the progenitor of
Cloud Computing in its modern form.  They expressly take the utility approach to
computing in the Cloud.  The development of the service was organic in that the
company needed a very high-powered computer system in order to deal with its
burgeoning success in the online sales industry.  Once it had developed the
system it needed, the company's founder Jeff Bezos realized that the potential
was there to open the system up in a metered way to the rest of the world.  This
was done both for monetary purposes (as the computing utility business model is
quite lucrative) and because it was a practical way for getting more information
into Amazon's systems.

*** Application

Today, Amazon (Reiss, 2008) and many others such as Yahoo (Yahoo, 2009), Google
(Baker, 2007), and Microsoft (Reiss, 2008) are beginning to offer Cloud
Computers under various guises.  Google, for instance, offers many software
services for free (such as a Mail Client, a Document editor, and a Spread Sheet
editor) while they insert their ad service into them (McDougall, 2007).  Beyond
just the cloud as a computer, there is a whole new breed of software termed
Software as a Service (SAAS) which offers no install, enterprise grade, software
services delivered via the cloud.  While this is not necessarily 'traditional'
cloud computing, it does offload the hardware and software requirements for
computer solutions to a central server rather than making any demands of local
hardware.

** Web 2.0

A central development that's helping to pave the way for netbook computers that
are streamlined for thin-client like computation utilizing Cloud Services is the
advent of what has come to be known as Web 2.0.  Tim O'Reilly, in his seminal
paper on the subject, describes Web 2.0 "as a set of principles and practices
that tie together a veritable solar system of sites that demonstrate some or all
of those principles" (2007). The principles he is referring to are things such
as an emphasis on deep customer collaboration, delivery of applications over the
Web rather than in a box, and an emphasis on multiple light-weight UIs to the
same data, especially the web connected devices.

*** The Web is the Platform

The "Web is the Platform" phrase is just ambiguous enough that it's incredibly
easy to misinterpret.  Tim O'Reilly notes this as well when he states that the
term has been adopted by "buzz-word addicted start-ups" with "no real
understanding of what it means" (2007).  The Web is the Platform, at its core,
means the use of the Internet to deliver services to customers (Miller, 2005).
This is in stark contrast to the model of 'shrink wrapped software' where a long
development cycle is concluded with a monolithic release that contains maybe a
year's worth of work available for repurchasing.  In that model, a customer
would buy a license to use a particular release of a software system forever and
would only purchase a new version if the features warranted it.  So in a strict
sense if a company uses the Web to deliver its software then it is using the Web
as a platform.  However, it would not, by the that single virtue, be very Web
2.0.

To be Web 2.0, a company or service provider must also pursue an aggressive
release schedule which delivers new features that are ready to be used on a
regular basis to customers (Miller, 2005).  This is what advocates of Web 2.0
say is the advantage of the Web as Platform; the ability to deliver features to
users of a given software system at a much higher rate (O'Reilly, 2007).

*** Key Properties

There are many key properties that have come to define Web 2.0.  Going back to
O'Reilly, it isn't possible to be strictly 'in' or 'out' of Web 2.0.  Rather,
software vendors are judged on a continuum regarding their adherence to the
general principles of Web 2.0 (2007).  Thus, company A could be very Web 2.0
wheras company B could exhibit some of the necessary qualities but in the end
still be very traditional in its approaches to business and service provision.
The following sections will define the core principles of Web 2.0 that have been
identified already.

There is a massive emphasis on collaboration at many levels in Web 2.0
(O'Reilly, 2007).  At the developer/vendor level, there is an emphasis on the
use of open tools and technologies to promote the maximum amount of
interoperability possible without literal reproduction of services (Miller,
2005).  For example, the social bookmarking site Delicious.com allows anyone to
access their data stream for any purpose (with the exception that no user of
that data create a search engine using it).  This could be as simple as
including a particular users latest delicious bookmarks on the homepage of their
blog or as complex as designing a popular tag portal that includes
advertisements based on the most popular tags clicked.  The point is,
Delicious.com opens up their data rather than creating a proprietary model that
attempts to lock out other services.

At the customer level, there is a new push for user generated content.  In the
words of Bryan Alexander, "these sections of the Web break away from the page
metaphor. Rather than following the notion of the Web as book, they are
predicated on microcontent" (2006).  In other words, rather than consumers doing
nothing but receieving packaged content, Web 2.0 emphasizes a partnership
between the primary content producer and the content consumers which yields a
change in the content being produced from which both primary consumers and
producers benefit.  The product is literally produced and changed as it is being
consumed (Alexander, 2006).  A key feature of Web 1.0 was that the customer was
seen primarily as a consumer of content rather than a participant in its
creation.  So even if a company had a dynamically created page with a database
back-end etc., they would still be in control of all of the variables in order
to create the user experience they desired.  Then, customers would use that
service if they liked what the company did or they would not.  In Web 2.0, that
same company would let the user have a much larger degree of control over their
experience with the service.

O'Reilly gives an example of this fundamental shift from Web 1.0 to Web 2.0 at
the time his paper was written: Barnes and Noble vs. Amazon (2007).  Barnes and
Noble does not offer fewer books than Amazon and it's just as recognizable
brand-wise, yet Amazon consistently out sells Barnes and Noble through their
online store.  O'Reilly believes that this is because Amazon specifically
encourages user participation in the form of product reviews, wish lists,
product recommendation lists (viewable by the public), and not the least of
which the use of their opinion to form search results (2007).  Whereas a search
at Barnes and Noble often "lead[s] with the company's own products, or sponsored
results, Amazon always leads with "most popular", a real-time computation based
not only on sales but other factors that Amazon insiders call the "flow" around
products" (O'Reilly, 2007).

Closely associated with collaboration, the principle of openness is primarily in
reference to data.  In the seven discreet core competencies of Web 2.0
organizations that O'Reilly documents, he lists "control over unique,
hard-to-recreate data sources that get richer as more people use them."
However, control here does not imply hiding (2007).  Instead, it means that the
corporations who set up the services that created the data control the
interfaces by which the data can be manipulated and added to. Again in the case
of Delicious, they have defined APIs for adding new bookmarks, accessing current
bookmarks, accessing popular bookmarks, etc.  This allows others to create new
views into that data that customers may find add enough value to the raw data
that they are willing to pay for access to the view.

Of course, the other part of the quote indicates that the data gets better as
more people use it.  Services like Delicious, Reddit, and Digg are so valuable
because of the amount of people using them.  It would be possible to create a
Delicious application that could only be used by a single person (i.e. the
bookmarking feature of any modern browser), but that wouldn't be very valuable
outside of any one person's use.  But if 100,000 people are all using a
bookmarking service, suddenly the information that can be derived from the
service is much more interesting to a much wider audience.  The more popular a
particular service is, the harder it becomes to reproduce that data set because
the market has shrunk.  In the words of O'Reilly, "the race is on to own certain
classes of core data."  Once the data is safely centralized under the auspices
of a partcular company it's then free to be exposed to others, allowing them to
create other interfaces (2007).

A third principle at the core of Web 2.0 is the delivery of software quickly
with an emphasis on streamlining for a specific purpose rather than creating the
next Microsoft Office or Adobe Creative Suite (Miller, 2005).  A prime example
of this would be Google Maps.  Maps has an extremely comprehensive data back end
but the interface to that data that Google designed is focused tightly on
getting directions to and from locations and viewing data about locations and
routes.  No attempts are made to shoehorn atlas type data or historical
information onto the location.  However, they have exposed an API to that map
data and other people or businesses are free to attempt to build that
information over the map information.  Thus, the business model of today's Web
2.0 corporations is adding value to data through the interface.

Because the data sources are being exposed to other businesses and to the
public, it's possible to create what has become known as mashup software.  This
is software that doesn't create any data of its own, but instead aggregates data
in an interesting way that may or may not be more useful to the customer.  One
company provides a comprehensive map database with an API, another company
provides an atlas service, and another provides a location based history
service.  Finally, a fourth company comes along and does nothing but provide a
mashup of the 3, allowing customers to browse a location, view statistics such
as population density, and look at historical highlights.  That's the heart of
mashup software and a significant part of what defines Web 2.0.

*** Specific Applications

There are two business domains that have taken a special interest in Web 2.0 as
a way to enhance what they are offering to their customers: The Medical
Community and the library Community.

Dean Giustini (2006) discusses how changes brought about by Web 2.0 could be
utilized to greatly change the way the Medical community serves its customers in
his editorial in the BMJ.  He envisions blogs as a way to help the medical
community skim through the massive volumes of information published by their
colleagues in a focused way.  Citing one especially notable blog, Clinical Cases
and Images, he notes that not only is it a way to get a lot of information
cleanly and easily, but it also allows for the medical community to engage with
the information instantaneously.  The prominent Web 2.0 technology RSS is also
noted as a way for the medical community to engage with information in a simple
and organized fashion.  Finally, he notes that a true success would be
accomplished if the medical community would embrace a wiki technology and engage
with one another in the building up of medical knowledge that is accessible to
all.  This would be a massive shift in the attitude of the medical community
which today is focused on secrecy more than collaboration.  However, he feels
that a wiki edited and controlled by the experts of the field could possibly
"create optimal knowledge building opportunities for doctors."

Paul Miller discusses how Web 2.0 philosophies can be applied in the library
community in his Ariadne article (2005).  The core of the argument is that
instead of fearing the inherent openness and collaborative nature of the Web 2.0
philosophy, libraries and librarians should instead embrace these philosophies
to help their customers and each other more effectively.  Libraries are already
uniquely positioned with highly specialized and unique data sources that until
now have been controlled completely by them.  Miller proposes an effort to at
least build collaboration between libraries (an effort which has already begun
to take shape today with services such as that offered by SirsiDynix for a
global library catalog) (2005).  This would be more along the lines of business
to business collaboration.  Nevertheless, it would begin to help libraries
behave in a more Web 2.0 way.  Finally, he makes the suggestion that while
building inter-library collaboration is a good start, libraries need to utilize
every applicable service to make their incredible wealth of data easily
available to the customer.

* Summary/Conclusions

The thrust of all of the literature covered here is the design of computing
hardware specifically made to interact with a more powerful machine that lives
across the network.

Historically, the time sharing computing model developed when computing hardware
was too expensive to justify everyone having their own personal computer.  In
order to cope with the situation, first computers were signed up for exclusive
use by individuals or groups trying to solve a problem.  When that proved to be
unworkable and inefficient, the solution appeared to be allowing multiple users
to use one computer through remote access (Campbell-Kelly, 2004).  In time, with
the rapid decline of hardware prices, the PC was born and it suddenly became
viable and attractive to give every customer their own PC (Campbell-Kelly,
2004).  Over time, though, this has proved to be an inefficient deployment of
computing hardware as it is a nightmare to maintain and administer (Schmidt,
1999).  Instead, a small return to time sharing has already happened in certain
specific slices of the business world.  Thin-Client computing is essentially a
rehashing of time-sharing except that today's thin clients must be able to
display GUIs rather than just CLIs (Schmidt, 1999).  The same parallels are
being drawn between Cloud Computing and time-sharing, just on a much larger
scale.  Thin-clients today are primarily designed to work on an intranet.  Cloud
Computing could take that one step further and put a thin client on the Internet
accessing globally available hardware over today and tomorrows wireless
broadband services.  A mobile thin client device could be very effective in the
near future.

Cloud Computing is what is making this possible.  Cloud Computing has two major
sides: business and consumer.  On the one hand, Cloud Computing describes the
use of outsourced data centers and computing clusters rather than the effort to
build and administer locally owned ones.  Businesses such as Amazon have
tremendous infrastructure already set up and have somewhat recently begun to
farm it out.  Smaller shops who need more computing power than they want to
provide for themselves can then pay a utility like fee to Amazon and begin to
use their hardware.  On the consumer end, then, is the Web 2.0 applications that
are being developed and hosted on that hardware.  These are Cloud Computing
because an application that used to be hosted on the Desktop and delivered via
shrink-wrapped packages are now being delivered via the Web and utilized via a
browser.  Both sides of this benefit greatly by the high-penetration of
broadband Internet access.  It's come to the point now that for day to day
usage, most people could get away comfortably with installing nothing on their
machine and instead accessing everything via the web, almost like a thin-client.

Netbooks are a step in that direction.  While not completely thin, they are
stripped down and designed to work with Internet applications rather than
today's power hungry local applications (Bergevin, 2006).  Netbooks are a cross
between the ultra-mobility of a PDA/Smartphone and the power and usability of a
notebook computer (Copeland, 2008).  However, their battery life is still
somewhat disappointing.  For instance, the ASUS Eee PC only gets about 6 hours
between charges.  It is the author's belief that Netbooks will not completely
catch on until they last at least a day between charges, as most of today's
successful Smartphones already do.  Smartphones are one the other end of the
spectrum.  They have a much smaller form factor, they are lower powered even
than the Netbooks, and they have more custom (although open) OSes.  However,
they are a platform that many customers are already using to access the services
they use that live in the Cloud.  If there isn't an application written
specifically for that service on the particular device owned by a customer, the
service can still be accessed via the web interfaces through the built in
browser.  Despite this, the interface is constrictive enough (common complaints
being the keyboards) that the author believes Netbook sized computers would be a
better solution.  With the problem lying in battery life, this paper
investigated current and historical trends in battery technology.  Lithium-Ion
secondary battery technology has been around since 1976 and first became
commercially available in 1981 when Bell Labs successfully used a Graphite anode
with a Titanium Sulfide cathode (Samar, 1981).  Today's research efforts are
primarily in increasing the energy density through nano-technology and cathode
materials.  Already, batteries have been produced that have 10 times the energy
density of a comparable volume battery today (so a battery that lasts 2 hours
today would last 20) (Stanford, 2007).  This is very promising for every mobile
device.

In conclusion, the design of computers intended for general use is becoming
thinner as more services are being offered over the Web via The Cloud.

* References

Alexander, B. (2006). Web 2.0: New Wave of Innovation for Teaching and
Learning?. EDUCAUSE Review, 41(2), 32-44

AP. (2009). Netbooks power down for portability. Retrieved 20 January, 2009,
from http://www.australianit.news.com.au/story/0,24897,24932480-5013037,00.html.

Baker, S. (2007). Google and the Wisdom of Clouds. Retrieved 13 January, 2009,
from
http://www.businessweek.com/print/magazine/content/07_52/b4064048925836.htm.

Bergevin, P. (2008). Thoughts on Netbooks. Retreived 3 March, 2009, from
http://blogs.intel.com/technology/2008/03/thoughts_on_netbooks.php.

Best, J. (2006). Analysis: What is a smart phone?. Retrieved 14 January, 2009,
from http://networks.silicon.com/mobile/0,39024665,39156391,00.htm.

Bleicher, P. (2006). Solutions Delivered, Not Installed. Applied Clinical
Trials, 15(6), 41-44.

Campbell-Kelly, M., & Aspray, W. (2004). Computer: A History of the Information
Machine. Boulder, CO: Westview Press.

Copeland, M. (2008). Disruptors: The 'netbook' revolution. Fortune
Magazine. Retrieved 15 January, 2009, from
http://money.cnn.com/2008/10/13/technology/copeland_asus.fortune/index.htm.

Ellis, B. l., Makahnouk, W. R. M., Makimura, Y., Toghill, K., Nazar,
L. F. (2007). A multifunctional 3.5V iron-based phosphate cathode for
rechargeable batteries.  Nature Materials, 6(10), 749-753.

Giustini, D. (2006). How Web 2.0 is changing medicine: Is a medical Wikipedia
the next step?. BMJ, 333(7582), 1283-4.

Idota, Y., Kubota, T., Matsufuji, A., Maekawa, Y., Miyasaka,
T. (1997). Tin-Based Amorphous Oxide: A High-Capacity Lithium-Ion-Storage
Material. Science Magazine, 276(5317), 1395-7.

Lai, A., Nieh, J. (2002) Limits of Wide-Area Thin-Client Computing. Proceedings
of the ACM Sigmetrics, 30(1), 228-239.

Levy, S. (1994). Hackers: Heroes of the Computer Revolution. New York, NY:
Penguin Books.

McDougall, P. (2007). Google Targets Microsoft With Launch Of Business
Applications. Retrieved 13 January, 2009, from
http://www.informationweek.com/news/internet/showArticle.jhtml?articleID=197007903.

Miller, P. (2005). Web 2.0: Building the New Library. Ariadne. 45. Retrieved 15
December, 2008 from http://www.ariadne.ac.uk/issue45/miller/

Nam, K. T., Kim, D., Yoo, P., Chiang, C., Meethong, N., Hammond, P., Chiang, Y.,
Belcher, A. (2006). Virus-Enabled Synthesis and Assembly of Nanowires for
Lithium Ion Battery Electrodes. Science, 312(5775), 885-8.

O'Reilly, T. (2007). What is Web 2.0: Design Patterns and Business Models for
the Next Generation of Software. Communications and Strategies, No. 1,
p. 17. Retrieved 19 September, 2008, from
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1008839

Reiss, S. (2008). Cloud Computing. Available at Amazon.com Today. Wired
Magazine, 16.05. Retrieved November 4, 2008, from
http://www.wired.com/techbiz/it/magazine/16-05/mf_amazon?currentPage=all

Samar, B. (1981). U.S. Patent No. 4304825A. European Patent Office:
http://v3.espacenet.com/publicationDetails/biblio?CC=US&NR=4304825A&KC=A&FT=D&date=19811208

Schmidt, B. K., Lam, M. S., Northcutt, J. D. (1999). The interactive performance
of SLIM: a stateless, thin-client architecture. Operating Systems Review, 34(5),
32-47.

Stanford University (2007, December 20). New Nanowire Battery Holds 10 Times The
Charge Of Existing Ones. ScienceDaily. Retrieved January 13, 2009, from
http://www.sciencedaily.com /releases/2007/12/071219103105.htm

Tynan, D. (2005). Think Thin. InfoWorld, 27(29), 32-36.

Yahoo (2009). Cloud Computing. Retrieved 12 January, 2009, from
http://research.yahoo.com/Cloud_Computing.
